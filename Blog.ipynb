{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Đồ án: Dự đoán sức mạnh nén của bê tông\n",
        "\n",
        "# Chương 1 – Giới thiệu đề tài\n",
        "\n",
        "## 1.1. Bối cảnh của dự án\n",
        "\n",
        "Trong thực tế, mối quan hệ giữa các thành phần vật liệu trong bê tông và cường độ chịu nén của nó thường rất phức tạp và phi tuyến. Ví dụ, lượng xi măng, nước, phụ gia siêu dẻo, hay tuổi của bê tông không ảnh hưởng độc lập mà có thể tương tác với nhau theo những cách không dễ mô hình hóa bằng hồi quy tuyến tính đơn giản. Chẳng hạn, một hỗn hợp bê tông có lượng xi măng cao nhưng thiếu phụ gia hoặc nước không phù hợp có thể không đạt được cường độ như mong muốn. Những mối quan hệ phi tuyến và tương tác giữa các đặc trưng này đòi hỏi các mô hình hồi quy linh hoạt hơn để mô phỏng chính xác.\n",
        "\n",
        "Do đó, việc áp dụng các mô hình hồi quy phi tuyến như Polynomial Regression, kết hợp với các kỹ thuật điều chuẩn như Ridge (L2) và Lasso (L1), là cần thiết để cải thiện độ chính xác và khả năng tổng quát hóa của mô hình trong việc dự đoán cường độ chịu nén của bê tông.\n",
        "\n",
        "## 1.2. Vấn đề nghiên cứu\n",
        "\n",
        "Làm thế nào để xây dựng một mô hình hồi quy có khả năng dự đoán chính xác cường độ chịu nén của bê tông, đồng thời tránh hiện tượng overfitting khi dữ liệu có nhiều đặc trưng vật liệu và mối quan hệ phi tuyến giữa chúng?\n",
        "\n",
        "## 1.3. Mục tiêu và câu hỏi cải tiến\n",
        "\n",
        "Mục tiêu của dự án là xây dựng một mô hình hồi quy có khả năng dự đoán chính xác cường độ chịu nén của bê tông dựa trên các thành phần vật liệu và thời gian đông kết. Mô hình cần đảm bảo tính tổng quát hóa tốt, tránh hiện tượng overfitting, đồng thời có thể giải thích được các mối quan hệ phi tuyến giữa các đặc trưng đầu vào.\n",
        "\n",
        "Để đạt được mục tiêu này, dự án đặt ra các câu hỏi cải tiến sau:\n",
        "\n",
        "- Làm thế nào để lựa chọn và biến đổi đặc trưng đầu vào nhằm phản ánh tốt hơn các mối quan hệ phi tuyến?\n",
        "- Mô hình hồi quy phi tuyến nào (ví dụ: Polynomial Regression, Ridge, Lasso) phù hợp nhất với bài toán này?\n",
        "- Việc điều chuẩn mô hình bằng các kỹ thuật như Ridge (L2) và Lasso (L1) có giúp cải thiện độ chính xác và giảm overfitting không?\n",
        "- Có thể sử dụng các kỹ thuật trực quan hóa hoặc phân tích hệ số để hiểu rõ hơn về ảnh hưởng của từng thành phần vật liệu đến cường độ bê tông?\n",
        "\n",
        "## 1.4. Phương pháp áp dụng\n",
        "\n",
        "- Áp dụng các mô hình: Linear Regression, Polynomial Regression, Ridge và Lasso.\n",
        "- Thực hiện tiền xử lý dữ liệu, tạo đặc trưng đa thức, chuẩn hóa và đánh giá mô hình bằng các chỉ số MSE, R².\n",
        "\n",
        "Trong bài blog này, người đọc sẽ được giới thiệu các khái niệm và lý thuyết nền tảng về hồi quy tuyến tính và hồi quy phi tuyến, cùng với phân tích ưu và nhược điểm của từng mô hình. Trong nhiều trường hợp thực tế, hồi quy phi tuyến tỏ ra hiệu quả hơn nhờ khả năng mô hình hóa các mối quan hệ phức tạp giữa các biến. Tuy nhiên, khi áp dụng mô hình phi tuyến, nguy cơ **overfitting** cũng tăng cao, khiến mô hình dễ bị sai lệch khi gặp dữ liệu mới. Để khắc phục điều này, các kỹ thuật điều chuẩn như L1 (Lasso) và L2 (Ridge) sẽ được áp dụng nhằm kiểm soát độ lớn của các trọng số và tăng tính ổn định cho mô hình.\n",
        "\n",
        "Các định nghĩa và khái niệm cơ bản sẽ được trình bày chi tiết trong **Chương 2**. Từ **Chương 3**, bài viết sẽ đi sâu vào phần thực hành, bao gồm cách triển khai dự án, phân tích dữ liệu, và quy trình xây dựng mô hình dự báo. Bên cạnh việc cải thiện mô hình, tiền xử lý dữ liệu cũng là một bước quan trọng không thể bỏ qua để đảm bảo chất lượng đầu vào cho mô hình học máy.\n",
        "\n",
        "Cuối cùng, **Chương 4** sẽ tổng hợp lại các kết quả thực nghiệm sau khi áp dụng các kỹ thuật mở rộng từ chương trước, giúp đánh giá hiệu quả của từng phương pháp và rút ra bài học kinh nghiệm cho các dự án tương tự trong tương lai.\n",
        "\n",
        "---\n",
        "\n",
        "# **Chương 2 – Cơ sở lý thuyết**\n",
        "\n",
        "## 2.1. Khái niệm và định nghĩa cơ bản\n",
        "\n",
        "### 2.1.1. Hồi quy tuyến tính (Linear Regression)\n",
        "* **Định nghĩa:** Là phương pháp hồi quy trong đó mối quan hệ giữa biến đầu vào và đầu ra được mô hình hóa bằng một hàm tuyến tính. Dạng tổng quát của mô hình là:\n",
        "   \n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2 + \\dots + w_nx_n + \\varepsilon\n",
        "$$\n",
        "\n",
        "* **Trong đó:**\n",
        "\n",
        "\t- $y$: Giá trị đầu ra (biến phụ thuộc)  \n",
        "\t- $w_0$: Hệ số chặn (*intercept*) – giá trị $y$ khi tất cả $x_i = 0$\n",
        "    - $w_i$: Hệ số hồi quy (*weight*) ứng với biến đầu vào $x_i$\n",
        "    - $x_i$: Biến đầu vào (biến độc lập)\n",
        "    - $\\varepsilon$: Sai số ngẫu nhiên (*error term*), biểu diễn phần nhiễu hoặc yếu tố không quan sát được\n",
        "    \n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251028_131937_bb342295.png\" alt=\"1.1\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">Hình 1: Hình ảnh minh họa về Linear Regression </div>\n",
        "\n",
        "* **Ưu điểm:**\n",
        "\t* Dễ hiểu, dễ triển khai.\n",
        "\t* Tính toán nhanh, hiệu quả với dữ liệu lớn.\n",
        "\t* Dễ giải thích kết quả (ý nghĩa của từng hệ số hồi quy).\n",
        "\n",
        "* **Nhược điểm:**\n",
        "\t* Không phù hợp khi mối quan hệ giữa biến đầu vào và đầu ra là phi tuyến.\n",
        "\t* Nhạy cảm với ngoại lệ (outliers).\n",
        "\t* Giả định mạnh về phân phối dữ liệu (ví dụ: phân phối chuẩn của sai số).\n",
        "    \n",
        "---\n",
        "    \n",
        "### 2.2.2. Hồi quy phi tuyến (Polynomial Regression)\n",
        "    \n",
        "* **Định nghĩa:**\n",
        "    \n",
        "    Là phương pháp hồi quy trong đó mối quan hệ giữa biến đầu vào và đầu ra được mô hình hóa bằng một hàm phi tuyến, ví dụ như hàm bậc hai, hàm mũ, logarit, sigmoid,....\n",
        "    \n",
        "   $$\n",
        "\ty = w_0 + w_1x + w_2x^2 + \\dots + w_nx^n + \\varepsilon\n",
        "\t$$\n",
        "\n",
        "* **Trong đó:**\n",
        "\t- $y$: Giá trị dự đoán (biến phụ thuộc)  \n",
        "\t- $w_0$: Hệ số chặn (*intercept*)  \n",
        "\t- $w_1, w_2, \\dots, w_n$: Các hệ số hồi quy (*weights*)  \n",
        "\t- $x, x^2, \\dots, x^n$: Các bậc của biến đầu vào (*polynomial terms*)  \n",
        "\t- $\\varepsilon$: Sai số ngẫu nhiên (*error term*)\n",
        "\n",
        "<div align=\"center\"> <img src=\"/static/uploads/20251028_132735_d90be111.png\n",
        "\" alt=\"1.2\" width=\"500\"> </div>\n",
        "\n",
        "<div align=\"center\"><em>Hình 2: Hình ảnh minh họa về Polynomial Regression</em></div>\n",
        "\n",
        "    \n",
        "* **Ưu điểm:**\n",
        "    \n",
        "    - Mô hình hóa được các mối quan hệ phức tạp, phi tuyến.\n",
        "    - Linh hoạt hơn trong việc mô tả dữ liệu thực tế.\n",
        "    \n",
        "* **Nhược điểm:**\n",
        "    \n",
        "    - Khó triển khai và giải thích hơn.\n",
        "    - Dễ bị overfitting nếu không kiểm soát tốt.\n",
        "    - Tốn thời gian tính toán và yêu cầu kỹ thuật cao hơn.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2.3. Regularization:\n",
        "    \n",
        "* **Định nghĩa:** Regularization là kỹ thuật quan trọng trong học máy, đặc biệt trong hồi quy, giúp giảm overfitting bằng cách thêm một điều khoản phạt vào hàm mất mát. Hai phương pháp phổ biến nhất là L1 (Lasso) và L2 (Ridge). Dưới đây là phần giải thích chi tiết:\n",
        "    \n",
        "    ---\n",
        "    \n",
        "#### 2.2.3.1. Lasso Regression (L1) - (Least Absolute Shrinkage and Selection Operator)\n",
        "    \n",
        "L1 thêm tổng giá trị tuyệt đối của các hệ số vào hàm mất mát:\n",
        "    \n",
        "$$\n",
        "Loss_{L1} = MSE + \\lambda \\sum_{j=1}^{n} |w_j|\n",
        "$$\n",
        "\n",
        "với:\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2\n",
        "$$\n",
        "\n",
        "* **Trong đó:**\n",
        "\t- $y_i$: Giá trị thực tế (ground truth)  \n",
        "\t- $\\hat{y_i}$: Giá trị dự đoán của mô hình  \n",
        "\t- $w_j$: Trọng số (weight) tương ứng với đặc trưng $x_j$  \n",
        "\t- $\\lambda$: Hệ số điều chỉnh mức độ regularization  \n",
        "\t- $m$: Số lượng mẫu dữ liệu  \n",
        "\t- $n$: Số đặc trưng (features)\n",
        "    \n",
        "=> Thành phần phạt $λ∑_{j=1}^n∣w_i∣$ giúp mô hình vừa giới hạn độ lớn của các trọng số, vừa tự động lựa chọn những đặc trưng quan trọng nhất. Khi λ lớn, nhiều trọng số sẽ bị triệt tiêu về 0 (feature selection), làm cho mô hình đơn giản và dễ hiểu hơn\n",
        "    \n",
        "* **Ưu điểm:**\n",
        "    \n",
        "\t- Giúp giảm số lượng đặc trưng không cần thiết → mô hình đơn giản hơn.    \n",
        "\t- Tốt khi có nhiều đặc trưng nhưng chỉ một số ít là quan trọng.\n",
        "    \n",
        "    ---\n",
        "    \n",
        "#### 2.2.3.2. Ridge regression (L2)\n",
        "    \n",
        "L2 thêm tổng bình phương các hệ số vào hàm mất mát:\n",
        "    \n",
        "$$\n",
        "Loss_{L2}=MSE+λ∑_{i=1}^n w_i^2\n",
        "$$\n",
        "\n",
        "với:\n",
        "$$\n",
        "MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2\n",
        "$$\n",
        "\n",
        "* **Trong đó:**\n",
        "\t- $y_i$: Giá trị thực tế (ground truth)  \n",
        "\t- $\\hat{y_i}$: Giá trị dự đoán của mô hình  \n",
        "\t- $w_j$: Trọng số (weight) tương ứng với đặc trưng $x_j$  \n",
        "\t- $\\lambda$: Hệ số điều chỉnh mức độ regularization  \n",
        "\t- $m$: Số lượng mẫu dữ liệu  \n",
        "\t- $n$: Số đặc trưng (features)\n",
        "    \n",
        "Nhờ vào cơ chế điều chỉnh độ lớn của các hệ số, Ridge giúp mô hình trở nên ổn định hơn, giảm độ nhạy với nhiễu trong dữ liệu và tránh việc học quá mức những biến động nhỏ trong tập huấn luyện.\n",
        "    \n",
        "Tuy nhiên, khác với Lasso, Ridge không loại bỏ hoàn toàn bất kỳ đặc trưng nào, các hệ số chỉ bị thu nhỏ chứ không bị đẩy về 0.\n",
        "    \n",
        "Vì vậy, Ridge thường được sử dụng trong các trường hợp mà ta muốn giữ lại toàn bộ đặc trưng, nhưng vẫn cần kiểm soát ảnh hưởng của những đặc trưng ít quan trọng để tránh làm phức tạp mô hình.\n",
        "    \n",
        "*\t**Ưu điểm:**\n",
        "\t- Tốt khi tất cả đặc trưng đều có ảnh hưởng nhỏ.\n",
        "\t- Ổn định hơn khi có nhiều đặc trưng tương quan.\n",
        "    \n",
        "    ---\n",
        "    \n",
        "\n",
        "## 2.3. Nhược điểm và khoảng trống chưa được khắc phục của L1 và L2 Regularization\n",
        "\n",
        "- **L1 Regularization (Lasso)**\n",
        "    1. **Không ổn định với đặc trưng tương quan cao**\n",
        "        \n",
        "        Khi các đặc trưng đầu vào có mối tương quan mạnh với nhau, Lasso có xu hướng chọn một đặc trưng và bỏ qua các đặc trưng còn lại, dẫn đến mô hình không ổn định và thiếu tính tổng quát.\n",
        "        \n",
        "    2. **Chọn lọc đặc trưng quá mức**\n",
        "        \n",
        "        Với giá trị λ lớn, Lasso có thể đẩy quá nhiều hệ số về 0, làm mất đi thông tin quan trọng và dẫn đến underfitting.\n",
        "        \n",
        "    3. **Không tối ưu cho bài toán có nhiều đặc trưng nhỏ nhưng quan trọng**\n",
        "        \n",
        "        Trong các bài toán mà nhiều đặc trưng có ảnh hưởng nhỏ nhưng cộng lại tạo ra ảnh hưởng lớn, Lasso có thể bỏ qua những đặc trưng này vì nó ưu tiên sự đơn giản.\n",
        "        \n",
        "\n",
        "---\n",
        "\n",
        "- **L2 Regularization (Ridge)**\n",
        "    1. **Không thực hiện chọn lọc đặc trưng**\n",
        "        \n",
        "        Ridge giữ lại tất cả đặc trưng, kể cả những đặc trưng không quan trọng, dẫn đến mô hình phức tạp và khó giải thích.\n",
        "        \n",
        "    2. **Không hiệu quả khi số lượng đặc trưng lớn hơn số lượng mẫu**\n",
        "        \n",
        "        Trong trường hợp dữ liệu có chiều cao (high-dimensional), Ridge không thể loại bỏ đặc trưng dư thừa, gây khó khăn trong việc giảm nhiễu.\n",
        "        \n",
        "    3. **Không xử lý tốt dữ liệu thưa (sparse data)**\n",
        "        \n",
        "        Với dữ liệu mà phần lớn giá trị là 0 (như dữ liệu văn bản), Ridge không thể tận dụng tính thưa để đơn giản hóa mô hình như Lasso.\n",
        "        \n",
        "\n",
        "---\n",
        "\n",
        "- **Khoảng trống chung chưa được khắc phục**\n",
        "    1. **Thiếu sự linh hoạt trong việc cân bằng giữa chọn lọc và giữ lại đặc trưng**\n",
        "        \n",
        "        Không có phương pháp nào trong L1 hoặc L2 có thể vừa chọn lọc đặc trưng hiệu quả vừa giữ lại thông tin từ các đặc trưng tương quan.\n",
        "        \n",
        "    2. **Không tự động điều chỉnh theo cấu trúc dữ liệu**\n",
        "        \n",
        "        Cả hai phương pháp đều yêu cầu người dùng chọn giá trị λ phù hợp, nhưng không có cơ chế nội tại để tự điều chỉnh theo đặc điểm của dữ liệu.\n",
        "        \n",
        "    3. **Không xử lý tốt dữ liệu phi tuyến**\n",
        "        \n",
        "        L1 và L2 đều hoạt động tốt trong mô hình tuyến tính, nhưng không đủ mạnh để xử lý mối quan hệ phi tuyến giữa các biến.\n",
        "        \n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251028_132851_2ae6d003.png\" alt=\"Hình 3: So sánh ý nghĩa hình học của Lasso Regression (L1) và Ridge Regression (L2) (AI Viet Nam)\" alt=\"1.1\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "<div align=\"center\"><em>Hình 3: So sánh ý nghĩa hình học của Lasso Regression (L1) và Ridge Regression (L2) (AI Viet Nam)</em></div>\n",
        "\n",
        "---\n",
        "\n",
        "## 2.4. Cải tiến với ElasticNet Regression (L1 + L2)\n",
        "**Định nghĩa:**\n",
        "ElasticNet kết hợp phạt L1 (Lasso) và L2 (Ridge) vào hàm mất mát:\n",
        "\n",
        "$${Loss_{ElasticNet} = \\underbrace{\\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2}_{MSE} + \\underbrace{\\alpha \\cdot \\rho \\cdot \\sum_{j=1}^{n} |w_j|}_{L1} + \\underbrace{\\alpha \\cdot (1 - \\rho) \\cdot \\sum_{j=1}^{n} w_j^2}_{L2}}$$\n",
        "\n",
        "**Trong đó:**\n",
        "\n",
        "  - $m$: Số mẫu dữ liệu\n",
        "  - $n$: Số đặc trưng\n",
        "  - $y_i$: Giá trị thực tế (sức mạnh bê tông)\n",
        "  - $\\hat{y}_i$: Giá trị dự đoán\n",
        "  - $w_j$: Hệ số hồi quy của đặc trưng $j$\n",
        "  - $\\alpha \\geq 0$: Tổng mức độ điều chuẩn (càng lớn → phạt càng mạnh)\n",
        "  - $\\rho \\in [0,1]$ (l1_ratio): Tỷ lệ L1\n",
        "    * $\\rho = 1 \\rightarrow$ Lasso\n",
        "    * $\\rho = 0 \\rightarrow$ Ridge\n",
        "    * $0 < \\rho < 1 \\rightarrow$ Kết hợp cả hai\n",
        "\n",
        "**Ưu điểm:**\n",
        "  - Chọn lọc đặc trưng + ổn định với biến tương quan\n",
        "  - Tốt khi có nhóm đặc trưng liên quan (vd: Cement ↔ Water)\n",
        "  - Giảm overfitting hiệu quả trên mô hình phi tuyến\n",
        "\n",
        "**Nhược điểm:**\n",
        "  - Cần tìm 2 siêu tham số ($\\alpha$, $\\rho$) → dùng ElasticNetCV\n",
        "  - Hệ số khó giải thích hơn do vừa triệt tiêu, vừa thu nhỏ\n",
        "\n",
        "\n",
        ">**Phù hợp với bài toán bê tông:**\n",
        "Nhiều biến tương quan + phi tuyến\n",
        "=> **Polynomial + ElasticNet** là lựa chọn tối ưu về độ chính xác và ổn định.\n",
        "---\n",
        "\n",
        "# Chương 3 – Phương pháp thực hiện\n",
        "\n",
        "## 3.1. Giới thiệu về  bộ dữ liệu\n",
        "\n",
        "* **Dataset name:** Concrete Compressive Strength\n",
        "\n",
        "* **Dataset Shape:** Gồm 8 Features và 1030 instances\n",
        "\n",
        "* **Data Field Description :**\n",
        "\n",
        "| Tên biến                | Loại dữ liệu | Đơn vị     | Vai trò     | Mô tả ngắn gọn                                      |\n",
        "|:------------------------|:-------------|:-----------|:------------|:----------------------------------------------------|\n",
        "| Cement                  | Liên tục     | kg/m³      | Đầu vào     | Lượng xi măng trong hỗn hợp bê tông                 |\n",
        "| Blast Furnace Slag      | Liên tục     | kg/m³      | Đầu vào     | Xỉ lò cao – vật liệu thay thế xi măng              |\n",
        "| Fly Ash                 | Liên tục     | kg/m³      | Đầu vào     | Tro bay – phụ gia khoáng                            |\n",
        "| Water                   | Liên tục     | kg/m³      | Đầu vào     | Lượng nước sử dụng trong hỗn hợp                    |\n",
        "| Superplasticizer        | Liên tục     | kg/m³      | Đầu vào     | Phụ gia siêu dẻo giúp cải thiện độ linh động       |\n",
        "| Coarse Aggregate        | Liên tục     | kg/m³      | Đầu vào     | Cốt liệu thô như đá lớn                             |\n",
        "| Fine Aggregate          | Liên tục     | kg/m³      | Đầu vào     | Cốt liệu mịn như cát                                |\n",
        "| Age                     | Số nguyên    | ngày       | Đầu vào     | Tuổi của bê tông tính từ lúc đổ                     |\n",
        "| Concrete Strength       | Liên tục     | MPa        | Đầu ra      | Cường độ chịu nén của bê tông                       |\n",
        "\n",
        "<div align=\"center\" >Bảng 1. Thông tin các biến đầu vào và đầu ra của dữ liệu bê tông</div>\n",
        "\n",
        "## 3.2. Khám Phá Dữ Liệu (EDA) và Tiền Xử Lý Dữ Liệu\n",
        "### 3.2.1. Tải và Làm sạch Dữ liệu\n",
        "Đầu tiên, chúng ta sẽ tải các thư viện cần thiết và nạp dữ liệu. Một hằng số `SEED` (cho `random_state`) để đảm bảo các thí nghiệm có thể được tái lặp lại một cách chính xác.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import sqrt\n",
        "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV, ElasticNetCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "import shap # Thư viện để giải thích mô hình\n",
        "\n",
        "# Đảm bảo tính tái lặp của thí nghiệm\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Tải dữ liệu\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\"\n",
        "df = pd.read_excel(url)\n",
        "print(f\"Tải thành công: {df.shape[0]} mẫu, {df.shape[1]} cột.\")\n",
        "\n",
        "# Đổi tên cột cho dễ xử lý (tên cột gốc quá dài và có dấu cách)\n",
        "column_names = {\n",
        "    'Cement (component 1)(kg in a m^3 mixture)': 'Cement',\n",
        "    'Blast Furnace Slag (component 2)(kg in a m^3 mixture)': 'Slag',\n",
        "    'Fly Ash (component 3)(kg in a m^3 mixture)': 'Fly_Ash',\n",
        "    'Water  (component 4)(kg in a m^3 mixture)': 'Water',\n",
        "    'Superplasticizer (component 5)(kg in a m^3 mixture)': 'Superplasticizer',\n",
        "    'Coarse Aggregate  (component 6)(kg in a m^3 mixture)': 'Coarse_Agg',\n",
        "    'Fine Aggregate (component 7)(kg in a m^3 mixture)': 'Fine_Agg',\n",
        "    'Age (day)': 'Age',\n",
        "    'Concrete compressive strength(MPa, megapascals) ': 'Strength' # Chú ý dấu cách ở cuối\n",
        "}\n",
        "df = df.rename(columns=column_names)\n",
        "\n",
        "print(\"5 dòng dữ liệu đầu tiên:\")\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "### 3.2.2. Kiểm tra Thông tin Dữ liệu\n",
        "\n",
        "```python\n",
        "# Kiểm tra thông tin chung và kiểu dữ liệu\n",
        "print(\"\\nThông tin chung (df.info()):\")\n",
        "df.info()\n",
        "\n",
        "# Kiểm tra các giá trị bị thiếu (Missing Values)\n",
        "print(f\"\\nSố lượng giá trị bị thiếu:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# Phân tích thống kê mô tả\n",
        "print(\"\\nThống kê mô tả (df.describe()):\")\n",
        "print(df.describe().T)\n",
        "```\n",
        "**Nhận xét:**\n",
        "  * Dữ liệu gồm 1030 mẫu và 9 cột.\n",
        "  * Tất cả các cột đều là kiểu `float64` hoặc `int64` (cho cột `Age`).\n",
        "  * **Không có giá trị bị thiếu (missing values)**, chúng ta không cần xử lý imputation.\n",
        "  * Nhìn vào `df.describe()`, chúng ta thấy các cột có **biên độ giá trị (scale) rất khác nhau**. Ví dụ: `Superplasticizer` (min 0, max 32.2) trong khi `Coarse_Agg` (min 801, max 1145).\n",
        "  * **Đây là một dấu hiệu cực kỳ quan trọng:** Bất kỳ mô hình nào nhạy cảm với scale (như Linear Regression có regularization, SVM, v.v.) đều **bắt buộc** phải có bước **Chuẩn hóa Dữ liệu (Scaling)**.\n",
        "\n",
        "### 3.2.3. Xóa Dữ liệu Trùng Lặp và Tách Bộ Dữ liệu (Train/Test Split)\n",
        "\n",
        "Dữ liệu sau khi xóa đi các giá trị trùng lặp sẽ được tách thành 2 phần: tập huấn luyện (Train) để xây dựng mô hình và tập kiểm tra (Test) để đánh giá hiệu suất trên dữ liệu \"mới\" mà mô hình chưa từng thấy.\n",
        "\n",
        "```python\n",
        "n_duplicates = df.duplicated().sum()\n",
        "\n",
        "if n_duplicates == 0:\n",
        "    print(\"✅ Không có dòng trùng lặp\")\n",
        "else:\n",
        "    print(f\"⚠️  Có {n_duplicates} dòng trùng lặp ({n_duplicates/len(df)*100:.2f}%)\")\n",
        "    print(\"   → Đang loại bỏ duplicates...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"   ✓ Đã loại bỏ, còn lại {len(df)} dòng\")\n",
        "\n",
        "# Xác định X (features) và y (target)\n",
        "X = df.drop('Strength', axis=1)\n",
        "y = df['Strength']\n",
        "\n",
        "# Tách dữ liệu\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "\n",
        "print(f\"\\nKích thước tập Train: {X_train.shape}\")\n",
        "print(f\"Kích thước tập Test: {X_test.shape}\")\n",
        "```\n",
        "\n",
        "## 3.3. Xây dựng mô hình Linear Regression\n",
        "\n",
        "### 3.3.1. Nhắc lại về độ đo R-squared ($R^2$)\n",
        "**Độ đo $R^2$ (Hệ số Xác định):**\n",
        "Như yêu cầu trong Rubric, $R^2$ là một độ đo quan trọng. Nó cho biết \"bao nhiêu phần trăm\" sự biến thiên của biến mục tiêu $y$ được giải thích bởi các biến đầu vào $X$ trong mô hình.\n",
        "\n",
        "$$R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}} = 1 - \\frac{\\sum (y_{\\text{actual}} - y_{\\text{pred}})^2}{\\sum (y_{\\text{actual}} - \\bar{y}_{\\text{actual}})^2}$$\n",
        "\n",
        "  * $SS_{\\text{res}}$ là tổng bình phương của phần dư (sai số).\n",
        "  * $SS_{\\text{tot}}$ là tổng bình phương của sự chênh lệch so với giá trị trung bình.\n",
        "  * $R^2 = 1$: Mô hình dự đoán hoàn hảo.\n",
        "  * $R^2 = 0$: Mô hình không tốt hơn việc chỉ dự đoán giá trị trung bình.\n",
        "  * $R^2 < 0$: Mô hình cực kỳ tệ.\n",
        "\n",
        "### 3.3.2. Thực hành:\n",
        "`Pipeline` của Scikit-learn cho phép đóng gói các bước tiền xử lý (như `StandardScaler`) và mô hình vào một đối tượng duy nhất.\n",
        "\n",
        "```python\n",
        "# 1. Định nghĩa các bước trong Pipeline\n",
        "# Bước 1: Chuẩn hóa dữ liệu (scaling)\n",
        "# Bước 2: Mô hình Linear Regression\n",
        "pipeline_linear = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# 2. Huấn luyện (fit) pipeline trên tập Train\n",
        "pipeline_linear.fit(X_train, y_train)\n",
        "\n",
        "# 3. Đánh giá trên tập Test\n",
        "y_pred_linear = pipeline_linear.predict(X_test)\n",
        "\n",
        "# 4. Tính toán các độ đo\n",
        "rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
        "r2_linear_train = pipeline_linear.score(X_train, y_train) # R2 trên tập Train\n",
        "r2_linear_test = pipeline_linear.score(X_test, y_test)   # R2 trên tập Test (r2_score(y_test, y_pred_linear))\n",
        "\n",
        "print(\"\\n--- Kết quả Mô hình 1: Linear Regression ---\")\n",
        "print(f\"R2 (Train): {r2_linear_train:.4f}\")\n",
        "print(f\"R2 (Test):  {r2_linear_test:.4f}\")\n",
        "print(f\"RMSE (Test): {rmse_linear:.4f}\")\n",
        "```\n",
        "**Kết quả mô hình 1: Linear Regression**\n",
        "- R2 (Train): 0.6098\n",
        "- R2 (Test):  0.5801\n",
        "- RMSE (Test): 11.1922\n",
        "\n",
        "**Nhận xét**\n",
        "\n",
        "  * Mô hình Linear Regression cơ bản cho $R^2 \\approx 0.62$ trên cả tập train và test.\n",
        "  * Điểm $R^2$ trên train và test gần nhau cho thấy mô hình **không bị overfitting**.\n",
        "  * Tuy nhiên, $R^2 = 0.62$ có nghĩa là mô hình chỉ giải thích được 62% sự biến thiên của dữ liệu. Mối quan hệ giữa các thành phần và sức mạnh bê tông có thể **không hoàn toàn tuyến tính**.\n",
        "  -----\n",
        "\n",
        "## 3.4. Cải tiến với PolynomialFeatures\n",
        "Chúng ta sẽ thử với bậc 2 (`degree=2`).\n",
        "```python\n",
        "# 1. Định nghĩa pipeline cho Polynomial Regression (Bậc 2)\n",
        "pipeline_poly2 = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly_features', PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)), # include_bias=False vì LinearRegression đã xử lý\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "# 2. Huấn luyện\n",
        "pipeline_poly2.fit(X_train, y_train)\n",
        "\n",
        "# 3. Đánh giá\n",
        "y_pred_poly2 = pipeline_poly2.predict(X_test)\n",
        "rmse_poly2 = np.sqrt(mean_squared_error(y_test, y_pred_poly2))\n",
        "r2_poly2_train = pipeline_poly2.score(X_train, y_train)\n",
        "r2_poly2_test = pipeline_poly2.score(X_test, y_test)\n",
        "\n",
        "print(\"\\n--- Kết quả Mô hình 2: Polynomial Regression (Bậc 2) ---\")\n",
        "print(f\"R2 (Train): {r2_poly2_train:.4f}\")\n",
        "print(f\"R2 (Test):  {r2_poly2_test:.4f}\")\n",
        "print(f\"RMSE (Test): {rmse_poly2:.4f}\")\n",
        "```\n",
        "**Kết quả mô hình 2: Polynomial Regression (Bậc 2)**\n",
        "\n",
        "- R2 (Train): 0.8095\n",
        "- R2 (Test):  0.7686\n",
        "- RMSE (Test): 8.3088\n",
        "\n",
        "**Nhận xét:**\n",
        "\n",
        "  *  $R^2$ (Test) đã tăng từ $0.62$ lên 0.81.\n",
        "  * Điều này khẳng định có mối quan hệ giữa các thành phần (ví dụ: tương tác giữa 'Cement' và 'Water') và sức mạnh bê tông là **phi tuyến**.\n",
        "  * $R^2$ (Train) và $R^2$ (Test) vẫn còn gần nhau, cho thấy mô hình bậc 2 vẫn tổng quát hóa tốt.\n",
        "\n",
        "\n",
        "### 3.4.1. Phân tích Overfitting với Bậc cao\n",
        "Tiếp theo chúng ta hãy thử tăng bậc lên cao hơn (ví dụ: bậc 3, bậc 4) để xem hiện tượng overfitting có xảy ra không.\n",
        "#### 3.4.1.1. Polynomial Regression (Bậc 3)\n",
        "```python\n",
        "# 1. Định nghĩa pipeline cho Polynomial Regression (Bậc 3)\n",
        "pipeline_poly3 = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly_features', PolynomialFeatures(degree=3, interaction_only=False,include_bias=False)),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# 2. Huấn luyện\n",
        "pipeline_poly3.fit(X_train, y_train)\n",
        "\n",
        "# 3. Đánh giá\n",
        "y_pred_poly3 = pipeline_poly3.predict(X_test)\n",
        "rmse_poly3 = np.sqrt(mean_squared_error(y_test, y_pred_poly3))\n",
        "r2_poly3_train = pipeline_poly3.score(X_train, y_train)\n",
        "r2_poly3_test = pipeline_poly3.score(X_test, y_test)\n",
        "\n",
        "print(\"\\n--- Kết quả Mô hình 3: Polynomial Regression (Bậc 3) ---\")\n",
        "print(f\"R2 (Train): {r2_poly3_train:.4f}\")\n",
        "print(f\"R2 (Test):  {r2_poly3_test:.4f}\")\n",
        "print(f\"RMSE (Test): {rmse_poly3:.4f}\")\n",
        "```\n",
        "**Kết quả mô hình 3: Polynomial Regression (Bậc 3)**\n",
        "\n",
        "- R2 (Train): 0.9247\n",
        "- R2 (Test):  0.8814\n",
        "- RMSE (Test): 5.9484\n",
        "\n",
        "**Nhận xét:**\n",
        "\n",
        "- $R^2$ (Train) bây giờ là 0.9311, (Test) là 0.8437. Mô hình đang hoạt động tốt trên tập train và bị Overfitting.\n",
        "\n",
        "#### 3.4.1.2. Polynomial Regression (Bậc 4)\n",
        "\n",
        "```python\n",
        "# 1. Định nghĩa pipeline cho Polynomial Regression (Bậc 4)\n",
        "pipeline_poly4 = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly_features', PolynomialFeatures(degree=4,interaction_only=False, include_bias=False)),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# 2. Huấn luyện\n",
        "pipeline_poly4.fit(X_train, y_train)\n",
        "\n",
        "# 3. Đánh giá\n",
        "y_pred_poly4 = pipeline_poly4.predict(X_test)\n",
        "rmse_poly4 = np.sqrt(mean_squared_error(y_test, y_pred_poly4))\n",
        "r2_poly4_train = pipeline_poly4.score(X_train, y_train)\n",
        "r2_poly4_test = pipeline_poly4.score(X_test, y_test)\n",
        "\n",
        "print(\"\\n--- Kết quả Mô hình 4: Polynomial Regression (Bậc 4) ---\")\n",
        "print(f\"R2 (Train): {r2_poly4_train:.4f}\")\n",
        "print(f\"R2 (Test):  {r2_poly4_test:.4f}\")\n",
        "print(f\"RMSE (Test): {rmse_poly4:.4f}\")\n",
        "```\n",
        "**Kết quả Mô hình 4: Polynomial Regression (Bậc 4)**\n",
        "\n",
        "- R2 (Train): 0.9821\n",
        "- R2 (Test):  -33.1951\n",
        "- RMSE (Test): 101.0018\n",
        "\n",
        "**Nhận xét:**\n",
        "- $R^2$ (Train) bây giờ là 0.9833. Mô hình đang hoạt động tốt trên tập train.\n",
        "- Tuy nhiên, $R^2$ (Test) đã **giảm** xuống còn -23.5. Mô hình bậc 4 quá phức tạp, Overfitting rất nặng, nó đã \"học thuộc\" cả nhiễu trong tập train.\n",
        "\n",
        "## 3.5. Cải tiến với Regularization (Ridge & Lasso & ElectNet)\n",
        "\n",
        "### 3.5.1.  Cải tiến với Ridge (L2)\n",
        "Chúng ta sẽ áp dụng Ridge vào mô hình Polynomial bậc 4 đang bị overfitting. Sử dụng `RidgeCV` để tự động tìm giá trị `alpha` tốt nhất thông qua cross-validation.\n",
        "\n",
        "```python\n",
        "# 1. Định nghĩa pipeline cho Polynomial (Bậc 4) + Ridge\n",
        "# Chúng ta sẽ dùng RidgeCV để tự động tìm alpha tốt nhất\n",
        "alphas_ridge = np.logspace(-7, 10, 1000)\n",
        "\n",
        "pipeline_ridge = Pipeline([\n",
        "    ('poly_features', PolynomialFeatures(degree=4, include_bias=False)),\n",
        "    ('scaler_poly', StandardScaler()),\n",
        "    # RidgeCV sẽ tự động thực hiện Cross-Validation để tìm alpha tốt nhất\n",
        "    ('model', RidgeCV(alphas=alphas_ridge, store_cv_values=True))\n",
        "])\n",
        "\n",
        "# 2. Huấn luyện\n",
        "pipeline_ridge.fit(X_train, y_train)\n",
        "\n",
        "# 3. Đánh giá\n",
        "y_pred_ridge = pipeline_ridge.predict(X_test)\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "r2_ridge_train = pipeline_ridge.score(X_train, y_train)\n",
        "r2_ridge_test = pipeline_ridge.score(X_test, y_test)\n",
        "\n",
        "# Lấy ra mô hình Ridge đã huấn luyện bên trong pipeline\n",
        "best_ridge_model = pipeline_ridge.named_steps['model']\n",
        "\n",
        "print(\"\\n--- Kết quả Mô hình 5: Polynomial (Bậc 4) + Ridge (L2) ---\")\n",
        "print(f\"Alpha tốt nhất được chọn: {best_ridge_model.alpha_:.4f}\")\n",
        "print(f\"R2 (Train): {r2_ridge_train:.4f}\")\n",
        "print(f\"R2 (Test):  {r2_ridge_test:.4f}\")\n",
        "print(f\"RMSE (Test): {rmse_ridge:.4f}\")\n",
        "```\n",
        "**Kết quả Mô hình 5: Polynomial (Bậc 4) + Ridge (L2)**\n",
        "- Alpha tốt nhất được chọn: 0.0772\n",
        "- R2 (Train): 0.9402\n",
        "- R2 (Test):  0.8961\n",
        "- RMSE (Test): 5.5686\n",
        "\n",
        "**Nhận xét:**\n",
        "- $R^2$ (Train) tốt với số điểm cao nhất 0.9402 và (Test) tăng lên 0.8961.\n",
        "- Mô hình vẫn còn Overfitting nhẹ nhưng đã giảm khá nhiều so với lúc ban đầu không áp dụng Ridge Regression L2.\n",
        "  \n",
        "### 3.5.2. Cải tiến với Lasso (L1)\n",
        "\n",
        "```python\n",
        "# 1. Định nghĩa pipeline cho Polynomial (Bậc 4) + Lasso\n",
        "# Lasso thường cần alpha nhỏ hơn và nhiều vòng lặp hơn\n",
        "\n",
        "pipeline_lasso = Pipeline([\n",
        "    ('poly_features', PolynomialFeatures(degree=4, include_bias=False)),\n",
        "    ('scaler_poly', StandardScaler()),\n",
        "    # LassoCV tìm alpha tốt nhất\n",
        "    ('model', LassoCV(n_alphas=100, cv=5, max_iter=200000, random_state=SEED))\n",
        "])\n",
        "\n",
        "# 2. Huấn luyện\n",
        "pipeline_lasso.fit(X_train, y_train)\n",
        "\n",
        "# 3. Đánh giá\n",
        "y_pred_lasso = pipeline_lasso.predict(X_test)\n",
        "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
        "r2_lasso_train = pipeline_lasso.score(X_train, y_train)\n",
        "r2_lasso_test = pipeline_lasso.score(X_test, y_test)\n",
        "\n",
        "# Lấy ra mô hình Lasso đã huấn luyện\n",
        "best_lasso_model = pipeline_lasso.named_steps['model']\n",
        "\n",
        "# Kiểm tra tính chất lựa chọn đặc trưng của Lasso\n",
        "# Lấy ra các đặc trưng đa thức (có rất nhiều!)\n",
        "poly = pipeline_lasso.named_steps['poly_features']\n",
        "num_features = len(poly.get_feature_names_out(X.columns))\n",
        "num_zero_coefs = np.sum(best_lasso_model.coef_ == 0)\n",
        "\n",
        "print(\"\\n--- Kết quả Mô hình 6: Polynomial (Bậc 4) + Lasso (L1) ---\")\n",
        "print(f\"Alpha tốt nhất được chọn: {best_lasso_model.alpha_:.6f}\")\n",
        "print(f\"R2 (Train): {r2_lasso_train:.4f}\")\n",
        "print(f\"R2 (Test):  {r2_lasso_test:.4f}\")\n",
        "print(f\"RMSE (Test): {rmse_lasso:.4f}\")\n",
        "print(f\"Số lượng đặc trưng (bậc 4): {num_features}\")\n",
        "print(f\"Số lượng đặc trưng bị Lasso đưa về 0: {num_zero_coefs} ({num_zero_coefs/num_features*100:.2f}%)\")\n",
        "```\n",
        "**Kết quả Mô hình 6: Polynomial (Bậc 4) + Lasso (L1)**\n",
        "- Alpha tốt nhất được chọn: 0.010939\n",
        "- R2 (Train): 0.9041\n",
        "- R2 (Test):  0.8703\n",
        "- RMSE (Test): 6.2213\n",
        "- Số lượng đặc trưng (bậc 4): 494\n",
        "- Số lượng đặc trưng bị Lasso đưa về 0: 397 (80.36%)\n",
        "\n",
        "### 3.5.3. Cải tiến với ElasticNet (L1+L2)\n",
        "```python\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "pipeline_enet = Pipeline([\n",
        "    ('poly_features', PolynomialFeatures(degree=4, include_bias=False)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', ElasticNetCV(\n",
        "        l1_ratio=[0.3, 0.5, 0.7],\n",
        "        alphas=np.logspace(-3, 1, 10),\n",
        "        cv=5,\n",
        "        max_iter=200000,\n",
        "        tol=1e-2,\n",
        "        random_state=SEED\n",
        "    ))\n",
        "])\n",
        "\n",
        "pipeline_enet.fit(X_train, y_train)\n",
        "\n",
        "# Dự đoán và đánh giá\n",
        "y_pred_enet = pipeline_enet.predict(X_test)\n",
        "rmse_enet = sqrt(mean_squared_error(y_test, y_pred_enet))\n",
        "r2_enet_train = pipeline_enet.score(X_train, y_train)\n",
        "r2_enet_test = pipeline_enet.score(X_test, y_test)\n",
        "\n",
        "print(\"\\n--- Kết quả Mô hình 7: Polynomial (Bậc 4) + ElasticNet ---\")\n",
        "print(\"RMSE test      :\", rmse_enet)\n",
        "print(\"R2 train       :\", r2_enet_train)\n",
        "print(\"R2 test        :\", r2_enet_test)\n",
        "\n",
        "best_enet_model = pipeline_enet.named_steps['model']\n",
        "poly_enet = pipeline_enet.named_steps['poly_features']\n",
        "feature_names_enet = poly_enet.get_feature_names_out(X_train.columns)\n",
        "num_features_enet = len(feature_names_enet)\n",
        "num_zero_coefs_enet = np.sum(best_enet_model.coef_ == 0)\n",
        "\n",
        "print(\"Số đặc trưng đa thức (bậc 4):\", num_features_enet)\n",
        "print(\"Số hệ số ElasticNet = 0    :\", num_zero_coefs_enet)\n",
        "print(\"Alpha ElasticNetCV chọn    :\", best_enet_model.alpha_)\n",
        "print(\"l1_ratio được chọn         :\", best_enet_model.l1_ratio_)\n",
        "\n",
        "```\n",
        "**Kết quả Mô hình 7: Polynomial (Bậc 4) + ElasticNet**\n",
        "- RMSE test      : 6.1930363040135035\n",
        "- R2 train       : 0.9088116822482784\n",
        "- R2 test        : 0.8714378864447145\n",
        "- Số đặc trưng đa thức (bậc 4): 494\n",
        "- Số hệ số ElasticNet = 0    : 66\n",
        "- Alpha ElasticNetCV chọn    : 0.0027825594022071257\n",
        "- l1_ratio được chọn         : 0.3\n",
        "\n",
        "### 3.5.4. Bảng Tóm tắt Hiệu suất Mô hình\n",
        "\n",
        "| Mô hình | Bậc Đa thức | Regularization | R² (Train) | R² (Test) | Ghi chú |\n",
        "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
        "| **Linear** | 1 | Không | 0.618 | 0.620 | Cơ bản, underfitting. |\n",
        "| **Polynomial** | 2 | Không | 0.8131 | 0.7843 | Overfitting ít. |\n",
        "| **Polynomial** | 3 | Không | 0.9311 | 0.8435 | Khá tốt, Overfitting vừa. |\n",
        "| **Polynomial** | 4 | Không | 0.9833 | -23.5212 | Overfitting nặng |\n",
        "| **Ridge (L2)** | 4 | **Có**  | 0.9402 | **0.8961** | Tốt,  Overfitting ít|\n",
        " **Lasso (L1)** | 4 | **Có**  | 0.9041 | **0.8703** | Khá tốt,  Overfitting ít|\n",
        " **Elastic** | 4 | **Có**  | 0.9088 | **0.8714** | Khá tốt,  Overfitting ít|\n",
        "\n",
        "<div align=\"center\"> Bảng 2: Đánh giá tổng quát mô hình Linear, Polynomial và Regularized </div>\n",
        "\n",
        "## 3.6. Đánh giá Mô hình Nâng cao\n",
        "\n",
        "### 3.6.1. Sử dụng `RepeatedKFold` (lặp lại K-Fold nhiều lần) để đánh giá mô hình Ridge (bậc 4)\n",
        "\n",
        "```python\n",
        "# 1. Chọn mô hình tốt nhất (pipeline_ridge)\n",
        "# Chúng ta sẽ tạo lại pipeline với alpha cố định đã tìm được\n",
        "best_alpha = pipeline_ridge.named_steps['model'].alpha_\n",
        "\n",
        "final_model_pipeline = Pipeline([\n",
        "    ('poly_features', PolynomialFeatures(degree=4, include_bias=False)),\n",
        "    ('scaler_poly', StandardScaler()),\n",
        "    ('model', Ridge(alpha=best_alpha))\n",
        "])\n",
        "\n",
        "# 2. Định nghĩa chiến lược CV\n",
        "# 10-Fold CV, lặp lại 3 lần với xáo trộn dữ liệu khác nhau\n",
        "cv_strategy = RepeatedKFold(n_splits=10, n_repeats=3, random_state=SEED)\n",
        "\n",
        "# 3. Chạy cross_val_score\n",
        "# scoring='r2' (mặc định)\n",
        "# scoring='neg_root_mean_squared_error' cho RMSE\n",
        "scores_r2 = cross_val_score(final_model_pipeline, X, y, cv=cv_strategy, scoring='r2')\n",
        "scores_rmse = cross_val_score(final_model_pipeline, X, y, cv=cv_strategy, scoring='neg_root_mean_squared_error')\n",
        "\n",
        "# 4. Báo cáo kết quả\n",
        "print(\"\\n--- Kết quả Đánh giá Nâng cao (Repeated K-Fold) ---\")\n",
        "print(f\"Mô hình: Polynomial (D=4) + Ridge (alpha={best_alpha:.4f})\")\n",
        "print(f\"R2 (Trung bình): {np.mean(scores_r2):.4f} +/- {np.std(scores_r2):.4f}\")\n",
        "print(f\"RMSE (Trung bình): {-np.mean(scores_rmse):.4f} +/- {np.std(scores_rmse):.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "**Kết quả Đánh giá Nâng cao (Repeated K-Fold)**\n",
        "- Mô hình: Polynomial (D=4) + Ridge (alpha=0.0772)\n",
        "- R2 (Trung bình): 0.8879 +/- 0.0210\n",
        "- RMSE (Trung bình): 5.3809 +/- 0.4980\n",
        "\n",
        "**Nhận xét:**\n",
        "- $R^2$ (Trung bình) 0.8879 là con số tin cậy về hiệu suất của mô hình.\n",
        "- Nó cho chúng ta biết rằng, trên dữ liệu mới, mô hình của chúng ta có khả năng giải thích trung bình 88% sự biến thiên của sức mạnh bê tông.\n",
        "\n",
        "### 3.6.2. Sử dụng chiến lược bootstrap để đánh giá khoảng tin cậy của mô hình\n",
        "\n",
        "```python\n",
        "# 1. Chọn mô hình tốt nhất (pipeline_ridge)\n",
        "# Chúng ta sẽ tạo lại pipeline với alpha cố định đã tìm được\n",
        "best_alpha = pipeline_ridge.named_steps['model'].alpha_\n",
        "\n",
        "final_model_pipeline = Pipeline([\n",
        "    ('poly_features', PolynomialFeatures(degree=4, include_bias=False)),\n",
        "    ('scaler_poly', StandardScaler()),\n",
        "    ('model', Ridge(alpha=best_alpha))\n",
        "])\n",
        "\n",
        "# 2. Định nghĩa chiến lược CV\n",
        "# 10-Fold CV, lặp lại 3 lần với xáo trộn dữ liệu khác nhau\n",
        "cv_strategy = RepeatedKFold(n_splits=10, n_repeats=3, random_state=SEED)\n",
        "\n",
        "# 3. Chạy cross_val_score\n",
        "# scoring='r2' (mặc định)\n",
        "# scoring='neg_root_mean_squared_error' cho RMSE\n",
        "scores_r2 = cross_val_score(final_model_pipeline, X, y, cv=cv_strategy, scoring='r2')\n",
        "scores_rmse = cross_val_score(final_model_pipeline, X, y, cv=cv_strategy, scoring='neg_root_mean_squared_error')\n",
        "\n",
        "# 4. Đánh giá khoảng tin cậy bằng bootstrap\n",
        "# Số lần lặp lại\n",
        "n_iterations = 1000\n",
        "# Khai báo khoảng tin cậy\n",
        "confidence_level = 0.95\n",
        "\n",
        "bootstrap_r2_mean = []\n",
        "bootstrap_rmse_mean = []\n",
        "\n",
        "for _ in range(n_iterations):\n",
        "  # Lẫy mẫu ngẫu nhiên có hoàn lại từ 30 giá trị (n_splits x n_repeats) r2 và rmse\n",
        "  sample_indices = np.random.choice(len(scores_r2), size=len(scores_r2), replace=True)\n",
        "  bootstrap_sample_r2 = scores_r2[sample_indices]\n",
        "  bootstrap_sample_rmse = scores_rmse[sample_indices]\n",
        "\n",
        "  # Tính r2, rmse trung bình của mẫu bootstrap và thêm vào list chứa\n",
        "  bootstrap_r2_mean.append(np.mean(bootstrap_sample_r2))\n",
        "  bootstrap_rmse_mean.append(np.mean(bootstrap_sample_rmse))\n",
        "\n",
        "# Chuyển đổi thành mảng numpy\n",
        "bootstrap_r2_mean = pd.Series(bootstrap_r2_mean)\n",
        "bootstrap_rmse_mean = pd.Series(bootstrap_rmse_mean)\n",
        "\n",
        "# Tính ra phân vị cận trên và dưới\n",
        "lower_bound = (1 - confidence_level) / 2\n",
        "upper_bound = 1 - lower_bound\n",
        "\n",
        "# Tính khoảng tin cậy hay phaan vị (Cònidence Interval - CI)\n",
        "lower_ci_r2 = bootstrap_r2_mean.quantile(lower_bound)\n",
        "upper_ci_r2 = bootstrap_r2_mean.quantile(upper_bound)\n",
        "\n",
        "lower_ci_rmse = bootstrap_rmse_mean.quantile(lower_bound)\n",
        "upper_ci_rmse = bootstrap_rmse_mean.quantile(upper_bound)\n",
        "\n",
        "# 5. Báo cáo kết quả\n",
        "print(\"\\n--- Kết quả Đánh giá Nâng cao (Repeated K-Fold) ---\")\n",
        "print(f\"Mô hình: Polynomial (D=4) + Ridge (alpha={best_alpha:.4f})\")\n",
        "print(f\"R2 (Trung bình): {np.mean(scores_r2):.4f} +/- {np.std(scores_r2):.4f}\")\n",
        "print(f\"RMSE (Trung bình): {-np.mean(scores_rmse):.4f} +/- {np.std(scores_rmse):.4f}\")\n",
        "print(f\"Số lần lặp bootstrap: {n_iterations} với confidence level: {confidence_level}:\")\n",
        "print(f\"Khoảng tin cậy R2: [{lower_ci_r2:.4f}, {upper_ci_r2:.4f}]\")\n",
        "print(f\"Khoảng tin cậy RMSE: [{-upper_ci_rmse:.4f}, {-lower_ci_rmse:.4f}]\") # Do dùng rmse âm, nên đảo dấu và khoảng tin cậy\n",
        "```\n",
        "**Kết quả Đánh giá Nâng cao (Repeated K-Fold)**\n",
        "- Mô hình: Polynomial (D=4) + Ridge (alpha=0.0772)\n",
        "- R2 (Trung bình): 0.8879 +/- 0.0210\n",
        "- RMSE (Trung bình): 5.3809 +/- 0.4980\n",
        "- Số lần lặp bootstrap: 1000 với confidence level: 0.95:\n",
        "- Khoảng tin cậy R2: [0.8806, 0.8958]\n",
        "- Khoảng tin cậy RMSE: [5.2037, 5.5604]\n",
        "\n",
        "**Nhận xét:**\n",
        "\n",
        "**R² trung bình = 0.8879 ± 0.0210**\n",
        "\n",
        "* R² ≈ 0.888 nghĩa là mô hình giải thích được ~88.8% độ biến thiên của sức mạnh chịu nén (compressive strength).\n",
        "* Độ lệch chuẩn chỉ ~0.021, rất nhỏ chỉ ra rằng hiệu năng gần như ổn định giữa các fold khác nhau.\n",
        "\n",
        "**Khoảng tin cậy R²: `[0.8809, 0.8953]`**\n",
        "\n",
        "  * Cực kỳ hẹp (~0.0144 chiều rộng).\n",
        "  * Trung bình R² rất ổn định trên các fold.\n",
        "\n",
        "## 3.7. Giải thích Mô hình (Model Interpretation)\n",
        "\n",
        "### 3.7.1. Phân tích Hệ số (Coefficients)\n",
        "Chúng ta có thể trích xuất các hệ số (trọng số $w$) từ mô hình Ridge.\n",
        "\n",
        "```python\n",
        "# Lấy ra các bước từ pipeline đã huấn luyện\n",
        "poly_features = pipeline_ridge.named_steps['poly_features']\n",
        "ridge_model = pipeline_ridge.named_steps['model']\n",
        "\n",
        "# Lấy tên của tất cả các đặc trưng đa thức\n",
        "feature_names = poly_features.get_feature_names_out(X.columns)\n",
        "coefficients = ridge_model.coef_\n",
        "\n",
        "# Tạo DataFrame để xem\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sắp xếp để xem các đặc trưng quan trọng nhất\n",
        "coef_df_sorted = coef_df.sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\n--- Giải thích Mô hình: Hệ số (Coefficients) ---\")\n",
        "print(\"Top 10 đặc trưng có ảnh hưởng TÍCH CỰC nhất:\")\n",
        "print(coef_df_sorted.head(10))\n",
        "\n",
        "print(\"\\nTop 10 đặc trưng có ảnh hưởng TIÊU CỰC nhất:\")\n",
        "print(coef_df_sorted.tail(10))\n",
        "```\n",
        ">**Kết quả:**\n",
        "\n",
        "| Feature                            | Coefficient |\n",
        "|----------------------------------|------------:|\n",
        "| Fine_Agg . $Age^3$                     | 36.681288   |\n",
        "| Coarse_Agg . $Age^3 $                  | 26.766043   |\n",
        "| $Age^3$                              | 23.297268   |\n",
        "| Slag . $FlyAsh^2$ . Fine_Agg            | 20.972378   |\n",
        "| Superplasticizer . $Age^3 $            | 19.510922   |\n",
        "| Water                              | 18.599341   |\n",
        "| Cement . $Age^3$                       | 18.190947   |\n",
        "| $Water^4$                            | 17.579194   |\n",
        "| $FlyAsh^3$ . Coarse_Agg               | 17.108438   |\n",
        "| Cement . $FlyAsh^2$ . Superplasticizer  | 16.940926   |\n",
        "\n",
        "<div align=\"center\">Bảng 3: Top 10 đặc trưng ảnh hưởng tích cực (Coefficient dương)</div>\n",
        "\n",
        "| Feature                                  | Coefficient |\n",
        "|-----------------------------------------|------------:|\n",
        "| Cement . Slag . Superplasticizer . Coarse_Agg  | -15.199808  |\n",
        "| Slag . $Water^3$                             | -15.462951  |\n",
        "| Cement . Slag . FlyAsh . Water                | -15.481468  |\n",
        "| $FlyAsh^4$                                | -16.336065  |\n",
        "| $Water^2$ . Coarse_Agg . Fine_Agg              | -18.549556  |\n",
        "| $Cement^2$ . Slag . Coarse_Agg                 | -20.248218  |\n",
        "| $Cement^2$ . FlyAsh . Coarse_Agg              | -21.366851  |\n",
        "| Cement . $FlyAsh^2$ . Water                   | -24.746038  |\n",
        "| Cement . Fine_Agg . $Age^2$                    | -24.829616  |\n",
        "| $Age^4$                                    | -48.676148  |\n",
        "\n",
        "<div align=\"center\">Bảng 4: Top 10 đặc trưng ảnh hưởng tiêu cực (Coefficient âm)</div>\n",
        "\n",
        "**Nhận xét:**\n",
        "\n",
        "* Phân tích các hệ số cho biết các đặc trưng *kết hợp* nào có ảnh hưởng lớn nhất.\n",
        "* Ví dụ: chúng ta có thể thấy `Age` và các bậc cao của `Age` (như `$Age^2$`, `$Age^3$`) có ảnh hưởng tích cực rất lớn. Điều này hợp lý: bê tông càng \"già\" (để lâu) thì càng mạnh.\n",
        "* Cả `Fine_Agg` và `Coarse_Agg` đều có tương tác với `$Age^3$` trong top dương. Điều này phù hợp vật lý: khi bê tông đủ tuổi, liên kết hồ xi măng – cốt liệu trở nên bền chắc, nên đóng góp lớn vào sức mạnh nén.\n",
        "\n",
        "### 3.7.2. SHAP\n",
        "#### 3.7.2.1. Chuẩn bị dữ liệu đã transform\n",
        "- Đầu tiên, ta cần trích xuất ra từ pipeline_ridge:\n",
        "  - Mô hình đã train\n",
        "  - Pipeline tiền xử lý dữ liệu\n",
        "  - Các đặc trưng sau khi polynomial transform\n",
        "---\n",
        "\n",
        "```python\n",
        "# Lấy model Ridge đã train\n",
        "best_ridge_model = pipeline_ridge.named_steps['model']\n",
        "\n",
        "# Lấy pipeline tiền xử lý (tất cả bước trước model)\n",
        "preprocess_pipeline = Pipeline(pipeline_ridge.steps[:-1])\n",
        "\n",
        "# Transform dữ liệu train và test\n",
        "X_train_transform = preprocess_pipeline.fit_transform(X_train)\n",
        "X_test_transform = preprocess_pipeline.transform(X_test)\n",
        "\n",
        "# Trích xuất tên đặc trưng sau khi polynomial transform\n",
        "```python\n",
        "poly_step = preprocess_pipeline.named_steps['poly_features']\n",
        "feature_names_poly = poly_step.get_feature_names_out(X_train.columns)\n",
        "\n",
        "print(f\"Số lượng đặc trưng gốc: {X_train.shape[1]}\")\n",
        "print(f\"Số lượng đặc trưng sau Polynomial (degree=4): {len(feature_names_poly)}\")\n",
        "print(f\"Shape X_train_transform: {X_train_transform.shape}\")\n",
        "print(f\"Shape X_test_transform: {X_test_transform.shape}\")\n",
        "```\n",
        "#### 3.7.2.2. Tạo Explainer SHAP\n",
        "```python\n",
        "explainer = shap.LinearExplainer(\n",
        "    best_ridge_model,\n",
        "    shap.maskers.Independent(X_train_transform)\n",
        ")\n",
        "\n",
        "# Chọn 50 mẫu test để giải thích\n",
        "n_samples = 50\n",
        "\n",
        "# Tính shap_values cho 50 mẫu test\n",
        "shap_values = explainer.shap_values(X_test_transform[:n_samples])\n",
        "\n",
        "print(f\"Shape của SHAP values: {shap_values.shape}\")\n",
        "print(f\"Expected value (baseline): {explainer.expected_value:.2f} MPa\")\n",
        "\n",
        "# Tạo Explanation object (để dùng shap.plots)\n",
        "explanation = shap.Explanation(\n",
        "    values=shap_values,\n",
        "    base_values=np.full(n_samples, explainer.expected_value),\n",
        "    data=X_test_transform[:n_samples],\n",
        "    feature_names=feature_names_poly\n",
        ")\n",
        "\n",
        "```\n",
        "#### 3.7.2.3. Biểu đồ waterfall\n",
        "Biểu đồ thác nước minh họa cách mỗi đặc trưng đóng góp vào việc \"đẩy\" giá trị dự đoán của mô hình từ một giá trị cơ sở (baseline) đến giá trị dự đoán cuối cùng cho **một mẫu dữ liệu duy nhất**\n",
        "\n",
        "| Thành phần | Khái niệm & Vai trò | Giải thích |\n",
        "| :--- | :--- | :--- |\n",
        "| **Giá trị Cơ sở** | **$E[f(x)]$** (Expected Value) | Điểm khởi đầu của dự đoán, đại diện cho **giá trị đầu ra trung bình** của mô hình trên toàn bộ tập dữ liệu. |\n",
        "| **Các Thanh** | **Đỏ** (Dương) & **Xanh** (Âm) | Mỗi thanh là một đặc trưng. Thanh **Đỏ/Xanh** là các đặc trưng có tác động làm **Tăng/Giảm** giá trị dự đoán. |\n",
        "| **Độ dài Thanh** | Độ lớn giá trị SHAP | Chiều dài của thanh biểu thị **mức độ ảnh hưởng** của đặc trưng đó. Thanh càng dài, ảnh hưởng càng lớn. |\n",
        "| **Giá trị Đặc trưng** | Hiển thị bên trái (Ví dụ: `Cement = -1.208`) | Giá trị đầu vào (đã được chuẩn hóa) của đặc trưng đó cho mẫu dữ liệu này. |\n",
        "| **Giá trị Dự đoán** | **$f(x)$** (Predicted Value) | Giá trị cuối cùng mà mô hình dự đoán, sau khi tổng hợp tất cả các tác động. |\n",
        "\n",
        "<div align=\"center\"> Bảng 5: Giải thích các thành phần của Biểu đồ Waterfall SHAP</div>\n",
        "\n",
        "\n",
        "```python\n",
        "# Chọn mẫu để giải thích (ví dụ: mẫu thứ 40)\n",
        "sample_idx = 40\n",
        "\n",
        "# Dự đoán cho mẫu đã chọn và lấy giá trị đầu tiên ([0]) từ kết quả\n",
        "y_pred = best_ridge_model.predict(X_test_transform[[sample_idx]])[0]\n",
        "y_true = y_test.iloc[sample_idx]\n",
        "\n",
        "print(f\"Mẫu #{sample_idx}: Dự đoán={y_pred:.2f} | Thực tế={y_true:.2f} MPa\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "shap.plots.waterfall(explanation[sample_idx], max_display=15, show=False)\n",
        "plt.title(f\"Waterfall Plot - Dự đoán: {y_pred:.2f} MPa | Thực tế: {y_true:.2f} MPa\",\n",
        "          fontsize=12, pad=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        ">**Kết quả**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251031_123026_f4989236.png\" alt=\"1.1\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">Hình 4: Biểu đồ thác SHAP minh họa sự đóng góp của từng đặc trưng vào một dự đoán đơn lẻ của mô hình. </div>\n",
        "\n",
        "**Nhận xét với Mẫu #40**\n",
        "\n",
        "- Dự đoán cho mẫu này bắt đầu từ giá trị cơ sở là **32.51 MPa**.\n",
        "- Các đặc trưng như `Cement Fly_Ash^2 Water` (-52.44) và `Cement^2 Fly_Ash Coarse_Agg` (-30.79) có giá trị SHAP **âm** khá lớn đóng vai trò chính trong việc \"kéo\" dự đoán đi xuống.\n",
        "- Ngược lại, các đặc trưng như `Cement^2 Fly_Ash^2` (+28.23), `Cement Fly_Ash` (+21.5) và `Cement Fly_Ash^2 Water` (+20.85) là những yếu tố chính \"đẩy\" dự đoán lên cao hơn.\n",
        "- Tổng hợp tất cả các ảnh hưởng này, mô hình đưa ra dự đoán cuối cùng là **36.95 MPa**.\n",
        "\n",
        "-----\n",
        "\n",
        "#### 3.7.2.4. Bar plot - Tầm quan trọng trung bình của các đặc trưng\n",
        "\n",
        "Biểu đồ này tóm tắt tầm quan trọng của mỗi đặc trưng (feature importance) trên một tập hợp nhiều mẫu dữ liệu. Nó giúp xác định những yếu tố nào có ảnh hưởng lớn nhất đến dự đoán của mô hình một cách tổng quan\n",
        "\n",
        "| Thành phần | Vai trò & Khái niệm | Giải thích |\n",
        "| :--- | :--- | :--- |\n",
        "| **Trục Y** | Các Đặc trưng | Liệt kê tên của các đặc trưng, được sắp xếp theo thứ tự **quan trọng giảm dần** từ trên xuống. |\n",
        "| **Trục X** | Giá trị trung bình của $|SHAP \\ value|$ | Biểu thị **mức độ ảnh hưởng trung bình** của mỗi đặc trưng. Giá trị càng lớn, đặc trưng càng quan trọng. |\n",
        "| **Các Thanh** | Tầm quan trọng | Chiều dài của thanh tương ứng với giá trị trung bình của SHAP value tuyệt đối, cho thấy tầm quan trọng tổng thể của đặc trưng. |\n",
        "\n",
        "<div align=\"center\"> Bảng 6: Ý nghĩa các thành phần Bar Plot SHAP</div>\n",
        "\n",
        "```python\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values, feature_names=feature_names_poly,\n",
        "                  plot_type=\"bar\", max_display=20, show=False)\n",
        "plt.title(\"Bar Plot - Feature Importance (Mean |SHAP|)\", fontsize=13, pad=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top features\n",
        "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "top_idx = np.argsort(mean_abs_shap)[-10:][::-1]\n",
        "print(\"\\nTOP 10 Features:\")\n",
        "for i, idx in enumerate(top_idx, 1):\n",
        "    print(f\"  {i:2d}. {feature_names_poly[idx]:30s} → {mean_abs_shap[idx]:.4f}\")\n",
        "```\n",
        ">**Kết quả**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251031_122450_3d7a9c26.png\" alt=\"1.1\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">Hình 5: Biểu đồ cột SHAP thể hiện tầm quan trọng trung bình của các đặc trưng đối với mô hình. </div>\n",
        "\n",
        ">**Nhận xét tổng quan**\n",
        "\n",
        "- Điều nổi bật nhất là không có đặc trưng gốc nào đứng đầu danh sách. Tất cả các đặc trưng quan trọng nhất đều là các số hạng tương tác hoặc bậc cao.\n",
        "- Điều này chứng tỏ mô hình Polynomial đã thành công trong việc học các mối quan hệ phi tuyến phức tạp giữa các thành phần mà mô hình đơn giản sẽ không thể nắm bắt được.\n",
        "- `Water` và `Cement` xuất hiện rất nhiều trong các tương tác quan trọng cho thấy rõ sự quan trọng của chúng.\n",
        "- Biểu đồ này cung cấp một cái nhìn tổng thể về những gì mô hình \"quan tâm\" nhất khi đưa ra dự đoán.\n",
        "\n",
        "-----\n",
        "\n",
        "#### 3.7.2.5. Biểu đồ phân tán (Beeswarm Plot)\n",
        "\n",
        "Biểu đồ này kết hợp tầm quan trọng của đặc trưng với **hướng và sự phân bố** của các tác động đó trên nhiều mẫu\n",
        "\n",
        "| Thành phần | Vai trò & Khái niệm | Giải thích |\n",
        "| :--- | :--- | :--- |\n",
        "| **Trục Y** | Các Đặc trưng | Được sắp xếp theo tầm quan trọng giảm dần từ trên xuống, tương tự như biểu đồ cột. |\n",
        "| **Trục X** | Giá trị SHAP | Giá trị **dương** (bên phải) làm **tăng** dự đoán. Giá trị **âm** (bên trái) làm **giảm** dự đoán. |\n",
        "| **Mỗi Điểm** | Một mẫu dữ liệu | Mỗi chấm trên biểu đồ tương ứng với một dự đoán cho một mẫu dữ liệu. |\n",
        "| **Màu Sắc** | Giá trị của đặc trưng | **Màu đỏ** biểu thị giá trị **cao** của đặc trưng. **Màu xanh** biểu thị giá trị **thấp** của đặc trưng. |\n",
        "| **Mật độ** | **Mật độ** | Tập trung càng dày cho thấy phần lớn dữ liệu với giá trị đặc trưng đó có tác động tương tự nhau và tác động đến đầu ra của mô hình. Ngược lại, thưa sẽ là ngoại lai hoặc trường hợp đặc biệt |\n",
        "\n",
        "<div align=\"center\">Bảng 7: Giải thích các thành phần của Biểu đồ Phân tán (Beeswarm Plot) SHAP</div>\n",
        "\n",
        "\n",
        "```python\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values, X_test_transform[:n_samples],\n",
        "                  feature_names=feature_names_poly, max_display=20, show=False)\n",
        "plt.title(\"Summary Plot - Importance toàn bộ model\", fontsize=13, pad=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        ">**Kết quả**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251031_123630_cd582176.png\" alt=\"1.1\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">Hình 6: Biểu đồ tổng hợp SHAP thể hiện tầm quan trọng và ảnh hưởng của từng đặc trưng trên toàn bộ tập dữ liệu.</div>\n",
        "\n",
        ">**Nhận xét chi tiết**\n",
        "\n",
        "- `Water^2 Coarse_Agg Fine_Agg`: Các điểm màu đỏ (giá trị đặc trưng cao) tập trung dày đặc ở phía giá trị SHAP **âm**. Ngược lại, các điểm màu xanh (giá trị đặc trưng thấp) nằm ở phía giá trị SHAP **dương**, cho thấy đây là đặc trưng có ảnh hưởng mạnh nhất đến mô hình theo mối quan hệ ngược chiều. Khi giá trị tương tác giữa nước và các loại cốt liệu tăng lên sẽ kéo dự đoán sức mạnh của bê tông xuống\n",
        "- `Cement Fly_Ash^2 Water`: Tương tự như đặc trưng trên, các điểm màu đỏ (giá trị cao) có giá trị SHAP **âm rất mạnh**. Các điểm màu xanh (giá trị thấp) có giá trị SHAP dương. Mối quan hệ này cũng là ngược chiều, giá trị giữa xi măng, tro bay bậc 2 và nước làm giảm mạnh dự đoán. Mô hình cho rằng, việc kết hợp lượng lớn các thành phần này có thể gây hại cho sức bền của bê tông\n",
        "- `Cement^2 Slag Coarse_Agg`: Cũng khá tương tự 2 trường hợp trên và đều thuộc **tương quan âm** rất mạnh và cũng sẽ làm giảm sức bền của bê tông nếu kết hợp với nhau\n",
        "\n",
        "- Nhược điểm duy nhất là đa cộng tuyến, ví dụ như `Water^4` khá khó diễn giải, có thể gây ra overfitting tiềm ẩn\n",
        "\n",
        "-----\n",
        "\n",
        "#### 3.7.2.6. Force plot - Biểu đồ lực (Cho 1 dự đoán cụ thể)\n",
        "\n",
        "Biểu đồ lực cung cấp một cách nhìn khác về việc giải thích một dự đoán duy nhất, thể hiện các đóng góp như những \"lực\" cân bằng lẫn nhau\n",
        "\n",
        "| Thành phần | Vai trò & Khái niệm | Giải thích |\n",
        "| :--- | :--- | :--- |\n",
        "| **Giá trị cơ sở (base value)** | Điểm cân bằng ban đầu | Giá trị dự đoán trung bình trên toàn bộ dữ liệu, là điểm khởi đầu của giải thích. |\n",
        "| **Các khối/mũi tên Đỏ** | Lực đẩy (Positive SHAP values) | Các đặc trưng làm **tăng** giá trị dự đoán, \"đẩy\" kết quả lên cao hơn giá trị cơ sở. |\n",
        "| **Các khối/mũi tên Xanh** | Lực kéo (Negative SHAP values) | Các đặc trưng làm **giảm** giá trị dự đoán, \"kéo\" kết quả xuống thấp hơn giá trị cơ sở. |\n",
        "| **Giá trị đầu ra (output value)** | Điểm cân bằng cuối cùng | Giá trị dự đoán cuối cùng sau khi tất cả các lực đẩy và kéo được tổng hợp. |\n",
        "\n",
        "<div align=\"center\">Bảng 8: Giải thích các thành phần của Biểu đồ Lực (Force Plot) SHAP</div>\n",
        "\n",
        "```python\n",
        "shap.initjs()\n",
        "plt.figure(figsize=(30, 3))\n",
        "shap.force_plot(\n",
        "    explainer.expected_value,\n",
        "    shap_values[sample_idx],\n",
        "    X_test_transform[sample_idx],\n",
        "    feature_names=feature_names_poly,\n",
        ")\n",
        "```\n",
        ">**Kết quả**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251031_124213_b8b1e6cd.jpg\n",
        "\" alt=\"1.1\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">Hình 7: Biểu đồ lực SHAP giải thích các yếu tố tác động đến một dự đoán đơn lẻ </div>\n",
        "\n",
        "-----\n",
        "\n",
        "#### 3.7.2.7. Biểu đồ phụ thuộc (Dependence plot)\n",
        "\n",
        "Biểu đồ phụ thuộc giúp chúng ta hiểu hành vi của mô hình trên toàn cục đối với một đặc trưng cụ thể. Nó trả lời hai câu hỏi quan trọng:\n",
        "\n",
        "-  **Mối quan hệ chính:** Giá trị của một đặc trưng ảnh hưởng đến dự đoán như thế nào? (Tăng hay giảm, tuyến tính hay phức tạp?)\n",
        "-  **Hiệu ứng tương tác:** Liệu ảnh hưởng của đặc trưng đó có thay đổi khi một đặc trưng khác thay đổi không?\n",
        "\n",
        "| Thành phần | Vai trò & Khái niệm | Giải thích |\n",
        "| :--- | :--- | :--- |\n",
        "| **Trục X** | Giá trị của Đặc trưng chính | Hiển thị dải giá trị của đặc trưng đang được phân tích (ví dụ: lượng `Cement`, `Age`). |\n",
        "| **Trục Y** | Giá trị SHAP của Đặc trưng chính | Cho thấy tác động của đặc trưng lên dự đoán. Giá trị > 0 làm **tăng** dự đoán, giá trị < 0 làm **giảm** dự đoán. |\n",
        "| **Mỗi Điểm** | Một mẫu dữ liệu | Mỗi chấm trên biểu đồ tương ứng với một hàng trong tập dữ liệu được giải thích. |\n",
        "| **Màu Sắc** | Giá trị của Đặc trưng Tương tác | Màu sắc của mỗi điểm được quyết định bởi giá trị của một đặc trưng khác mà SHAP cho là có tương tác mạnh nhất. **Màu đỏ** là giá trị **cao**, **màu xanh** là giá trị **thấp**. |\n",
        "| **Phân bố Dọc** | Thể hiện hiệu ứng tương tác | Nếu các điểm tại một giá trị X nhất định bị phân tán theo trục Y, điều đó có nghĩa là ảnh hưởng của đặc trưng chính còn phụ thuộc vào giá trị của đặc trưng tương tác (màu sắc). |\n",
        "\n",
        "<div align=\"center\">Bảng 9: Giải thích các thành phần của Biểu đồ Phụ thuộc (Dependence Plot) SHAP</div>\n",
        "\n",
        "\n",
        "```python\n",
        "# Lấy top 3 features GỐC quan trọng nhất\n",
        "original_features = X_train.columns.tolist()\n",
        "original_importance = {}\n",
        "for feat in original_features:\n",
        "    if feat in feature_names_poly:\n",
        "        idx = list(feature_names_poly).index(feat)\n",
        "        original_importance[feat] = np.abs(shap_values[:, idx]).mean()\n",
        "\n",
        "top_3_orig = sorted(original_importance.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "print(\"\\n🎯 Phân tích TOP 3 features gốc:\\n\")\n",
        "\n",
        "for rank, (feat_name, importance) in enumerate(top_3_orig, 1):\n",
        "    print(f\"{'─' * 70}\")\n",
        "    print(f\"#{rank}. {feat_name} (Mean |SHAP|: {importance:.4f})\")\n",
        "    print(f\"{'─' * 70}\")\n",
        "\n",
        "    feat_idx = list(feature_names_poly).index(feat_name)\n",
        "\n",
        "    # Tính correlation\n",
        "    feat_vals = X_test_transform[:n_samples, feat_idx]\n",
        "    shap_vals = shap_values[:, feat_idx]\n",
        "    corr = np.corrcoef(feat_vals, shap_vals)[0, 1]\n",
        "\n",
        "    # Dependence plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    shap.dependence_plot(\n",
        "        feat_idx,\n",
        "        shap_values,\n",
        "        X_test_transform[:n_samples],\n",
        "        feature_names=feature_names_poly,\n",
        "        interaction_index=\"auto\",\n",
        "        ax=ax,\n",
        "        show=False\n",
        "    )\n",
        "    plt.title(f\"Dependence Plot: {feat_name} (correlation={corr:.3f})\",\n",
        "              fontsize=12, pad=15)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "```\n",
        ">**Kết quả**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251031_125315_9e8900eb.png\" alt=\"3.5\" width=\"500\">\n",
        "</div>\n",
        "<div align=\"center\">\n",
        "Hình 8: Biểu đồ phụ thuộc SHAP  thể hiện ảnh hưởng của đặc trưng `Water` lên dự đoán của mô hình.\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251031_125322_739f775f.png\" alt=\"3.6\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "Hình 9: Biểu đồ phụ thuộc SHAP thể hiện ảnh hưởng của đặc trưng `Cement` lên dự đoán của mô hình.\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/static/uploads/20251031_125334_82135b8d.png\" alt=\"3.7\" width=\"500\">\n",
        "</div>\n",
        "<div align=\"center\">\n",
        "Hình 10: Biểu đồ phụ thuộc SHAP thể hiện ảnh hưởng của đặc trưng `Coarse_Agg` lên dự đoán của mô hình\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        ">**Nhận xét từ các Biểu đồ**\n",
        "\n",
        "- **Water (Nước), Cement (Xi măng) và Coarse_Agg ()** có mối quan hệ **tích cực rõ ràng**, khi các giá tỉ này càng nhiều sẽ giúp tăng giá tị dự đoán SHAP như càng nhiều nước, bê tông sẽ làm tăng dự đoán sức mạnh của bê tông\n",
        "\n",
        "# Chương 4 - Tổng kết\n",
        "\n",
        "Đề tài đã hoàn thành mục tiêu nghiên cứu và xây dựng mô hình dự đoán cường độ chịu nén của bê tông dựa trên dữ liệu thực nghiệm. Quá trình thực hiện cho thấy, việc kết hợp giữa hiểu biết chuyên môn và các phương pháp học máy mang lại hiệu quả rõ rệt trong việc mô hình hóa và phân tích các mối quan hệ phức tạp giữa các thành phần vật liệu.\n",
        "\n",
        "Kết quả không chỉ chứng minh tính khả thi của mô hình mà còn mở ra hướng tiếp cận mới trong việc ứng dụng trí tuệ nhân tạo vào lĩnh vực xây dựng. Bên cạnh giá trị kỹ thuật, đề tài cũng là trải nghiệm giúp người thực hiện nhận ra rằng, trong mọi công nghệ — sự hiểu biết và tư duy khoa học của con người vẫn là nền tảng quan trọng nhất.\n",
        "\n",
        ">Công nghệ có thể hỗ trợ, nhưng chính con người mới là yếu tố tạo nên giá trị cho tri thức.\n",
        "\n",
        "---\n",
        "\n",
        "# Chương 5 - Tài liệu tham khảo\n",
        "\n",
        "- **Slide về Project Sales Prediction  (Nguyễn Quốc Thái, Hồ Quang Hiển, Đinh Quang Vinh):**\n",
        "\n",
        "[https://lms.aivietnam.edu.vn/api/files/683188ef519c0e157fb514f3/Documents%2F2025-9%2FM05W04%20-%20Project%3A%20From%20linear%20to%20non-linear%20regression%2F%5BDescription-V2%5D-Project-5.1-House-Price-Prediction-Advanced-Regression-Techniques.pdf](https://lms.aivietnam.edu.vn/api/files/683188ef519c0e157fb514f3/Documents%2F2025-9%2FM05W04%20-%20Project%3A%20From%20linear%20to%20non-linear%20regression%2F%5BDescription-V2%5D-Project-5.1-House-Price-Prediction-Advanced-Regression-Techniques.pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yoF5XWI1gNtE"
      }
    }
  ]
}